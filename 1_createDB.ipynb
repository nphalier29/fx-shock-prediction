{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6391b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import de packages ---\n",
    "\n",
    "import pandas as pd\n",
    "import taceconomics\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "from gdeltdoc import GdeltDoc, Filters\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- API Key et dates ---\n",
    "\n",
    "taceconomics.api_key = \"sk_o24BhJRqVpIvxVSXX3yiKRGgpDEejmyJ8pfLFX2q22s\"  \n",
    "start_date = '2023-01-01'\n",
    "end_date = datetime.today().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "18cfe72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               close\n",
      "timestamp           \n",
      "2023-05-04  1.102256\n",
      "2023-05-05  1.121026\n",
      "2023-05-06  1.121026\n",
      "2023-05-07  1.101831\n",
      "2023-05-08  1.099578\n",
      "2023-05-09  1.096431\n",
      "2023-05-10  1.098370\n",
      "2023-05-11  1.091179\n",
      "2023-05-12  1.093967\n",
      "2023-05-13  1.093967\n",
      "(874, 1)\n",
      "Index(['close'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# --- Importation des données ---\n",
    "\n",
    "# Taux de change EUR/USD\n",
    "usd_eur = taceconomics.getdata(f\"EXR/EUR/WLD?start_date={start_date}\")\n",
    "usd_eur.columns = [\"usd_eur\"]\n",
    "usd_eur = usd_eur.dropna()\n",
    "eur_usd = 1 / usd_eur  # Inversion pour avoir EUR/USD\n",
    "eur_usd.columns = [\"close\"]\n",
    "eur_usd.index = pd.to_datetime(eur_usd.index)\n",
    "\n",
    "print(eur_usd.head(10))\n",
    "print(eur_usd.shape)\n",
    "print(eur_usd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ddabcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(874, 4)\n",
      "Index(['close', 'rendement_log', 'vol_30j', 'rendement_log_10j'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# --- Indicateurs de base ---\n",
    "\n",
    "# Rendement logarithmique\n",
    "eur_usd[\"rendement_log\"] = np.log(eur_usd[\"close\"] / eur_usd[\"close\"].shift(1))\n",
    "\n",
    "# Volatilité glissante (30 jours) basée sur les rendements log\n",
    "eur_usd[\"vol_30j\"] = eur_usd[\"rendement_log\"].rolling(window=30).std()\n",
    "\n",
    "# Rendement logarithmique cumulé à 10 jours\n",
    "eur_usd[\"rendement_log_10j\"] = np.log(eur_usd[\"close\"] / eur_usd[\"close\"].shift(10))\n",
    "\n",
    "# print(eur_usd.head(60))\n",
    "print(eur_usd.shape)\n",
    "print(eur_usd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "868dedf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(874, 6)\n",
      "Index(['close', 'rendement_log', 'vol_30j', 'rendement_log_10j',\n",
      "       'rendement_futur_10j', 'target'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_w/2y4ytldn4yg8zlk064ghy39w0000gn/T/ipykernel_29036/2658521818.py:4: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  eur_usd[\"rendement_futur_10j\"] = eur_usd[\"close\"].shift(-10).pct_change(periods=10)\n"
     ]
    }
   ],
   "source": [
    "# --- Cible --- \n",
    "\n",
    "# Rendement futur à 10 jours (≈ 2 semaines)\n",
    "eur_usd[\"rendement_futur_10j\"] = eur_usd[\"close\"].shift(-10).pct_change(periods=10)\n",
    "\n",
    "# Seuils de choc dynamiques (±2 × volatilité)\n",
    "seuil_haut = 2 * eur_usd[\"vol_30j\"]\n",
    "seuil_bas = -2 * eur_usd[\"vol_30j\"]\n",
    "\n",
    "# Target : 1 si choc de volatilité (hausse/baisse), 0 sinon\n",
    "eur_usd[\"target\"] = (\n",
    "    (eur_usd[\"rendement_futur_10j\"] >= seuil_haut) |\n",
    "    (eur_usd[\"rendement_futur_10j\"] <= seuil_bas)\n",
    ").astype(int)\n",
    "\n",
    "# print(eur_usd.head(60))\n",
    "print(eur_usd.shape)\n",
    "print(eur_usd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9c9c89b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(844, 10)\n",
      "Index(['close', 'rendement_log', 'vol_30j', 'rendement_log_10j', 'target',\n",
      "       'mm7', 'mm21', 'boll_haut', 'boll_bas', 'rsi_14j'],\n",
      "      dtype='object')\n",
      "close                 1.072559\n",
      "rendement_log         0.000000\n",
      "vol_30j               0.005411\n",
      "rendement_log_10j    -0.002793\n",
      "target                0.000000\n",
      "mm7                   1.072378\n",
      "mm21                  1.077441\n",
      "boll_haut             1.088046\n",
      "boll_bas              1.066056\n",
      "rsi_14j              33.354206\n",
      "Name: 2023-06-03 00:00:00, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- Indicateurs techniques ---\n",
    "\n",
    "# Moyennes mobiles\n",
    "eur_usd[\"mm7\"] = eur_usd[\"close\"].rolling(window=7).mean()\n",
    "eur_usd[\"mm21\"] = eur_usd[\"close\"].rolling(window=21).mean()\n",
    "\n",
    "# Bandes de Bollinger (20 jours)\n",
    "rolling_mean_20 = eur_usd[\"close\"].rolling(window=20).mean()\n",
    "rolling_std_20 = eur_usd[\"close\"].rolling(window=20).std()\n",
    "eur_usd[\"boll_haut\"] = rolling_mean_20 + 2 * rolling_std_20\n",
    "eur_usd[\"boll_bas\"] = rolling_mean_20 - 2 * rolling_std_20\n",
    "\n",
    "# RSI\n",
    "def calculer_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    gain = np.where(delta > 0, delta, 0)\n",
    "    perte = np.where(delta < 0, -delta, 0)\n",
    "    avg_gain = pd.Series(gain, index=series.index).rolling(window=window).mean()\n",
    "    avg_perte = pd.Series(perte, index=series.index).rolling(window=window).mean()\n",
    "    rs = avg_gain / avg_perte\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "eur_usd[\"rsi_14j\"] = calculer_rsi(eur_usd[\"close\"])\n",
    "\n",
    "# --- Nettoyage final ---\n",
    "\n",
    "eur_usd = eur_usd.drop(columns=[\"rendement_futur_10j\"])\n",
    "eur_usd = eur_usd.dropna().copy()\n",
    "\n",
    "# print(eur_usd.head(60))\n",
    "print(eur_usd.shape)\n",
    "print(eur_usd.columns)\n",
    "print(eur_usd.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc55801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eur_usd.to_excel(\"data/test_eur_usd_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b4032854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/noe/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération sentiment GDELT de 2023-06-03 à 2025-10-05\n",
      "date\n",
      "2023-06-03    0.283725\n",
      "2023-06-05   -0.116700\n",
      "2023-06-06   -0.440400\n",
      "2023-06-07   -0.057000\n",
      "2023-06-08   -0.256643\n",
      "2023-06-09    0.017650\n",
      "2023-06-10    0.000000\n",
      "2023-06-12   -0.160760\n",
      "2023-06-13   -0.027826\n",
      "2023-06-14   -0.018286\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- Indicateurs de sentiment ---\n",
    "\n",
    "# Télécharger VADER (une seule fois)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialiser VADER globalement\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def recuperer_sentiment_gdelt(start_date, end_date, keyword=\"EUR/USD\", language='eng', chunk_days=30, num_records=250):\n",
    "    \"\"\"\n",
    "    Récupère les articles GDELT pour un mot-clé donné entre deux dates,\n",
    "    calcule le score de sentiment pour chaque article, et agrège par jour.\n",
    "    \"\"\"\n",
    "    # Convertir les dates\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    gd = GdeltDoc()\n",
    "    all_articles = []\n",
    "\n",
    "    current_start = start\n",
    "    while current_start <= end:\n",
    "        current_end = min(current_start + timedelta(days=chunk_days), end)\n",
    "\n",
    "        # print(f\"Récupération articles: {current_start.date()} -> {current_end.date()}\")\n",
    "\n",
    "        # Créer filtre\n",
    "        f = Filters(\n",
    "            start_date=current_start.strftime(\"%Y-%m-%d\"),\n",
    "            end_date=current_end.strftime(\"%Y-%m-%d\"),\n",
    "            num_records=num_records,\n",
    "            keyword=keyword,\n",
    "            language=language\n",
    "        )\n",
    "\n",
    "        # Récupérer articles\n",
    "        try:\n",
    "            articles_df = gd.article_search(f)\n",
    "            if not articles_df.empty:\n",
    "                all_articles.append(articles_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur récupération: {e}\")\n",
    "\n",
    "        current_start = current_end + timedelta(days=1)\n",
    "\n",
    "    # Vérifier si on a récupéré des articles\n",
    "    if not all_articles:\n",
    "        print(\"Aucun article trouvé pour la période.\")\n",
    "        return pd.DataFrame(columns=['date', 'sentiment'])\n",
    "\n",
    "    df = pd.concat(all_articles, ignore_index=True)\n",
    "\n",
    "    # Parsing date GDELT\n",
    "    def convertir_date_gdelt(date_str):\n",
    "        try:\n",
    "            # Format AAAAMMJJTHHMMSSZ ou AAAAMMJJ\n",
    "            date_part = date_str.split('T')[0]\n",
    "            return datetime.strptime(date_part, \"%Y%m%d\").date()\n",
    "        except:\n",
    "            return pd.NaT\n",
    "\n",
    "    df['date'] = df['seendate'].apply(convertir_date_gdelt)\n",
    "    df = df.dropna(subset=['date'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Calculer sentiment pour chaque article\n",
    "    def calculer_sentiment(texte):\n",
    "        if isinstance(texte, str) and texte.strip():\n",
    "            return sid.polarity_scores(texte)['compound']\n",
    "        return 0\n",
    "\n",
    "    df['sentiment'] = df['title'].apply(calculer_sentiment)\n",
    "\n",
    "    # Agréger par jour : score moyen par jour\n",
    "    df_daily = df.groupby('date')['sentiment'].mean().reset_index()\n",
    "\n",
    "    return df_daily\n",
    "\n",
    "start_date = eur_usd.iloc[0].name.strftime(\"%Y-%m-%d\")\n",
    "end_date = eur_usd.iloc[-1].name.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Récupération sentiment GDELT de {start_date} à {end_date}\")\n",
    "\n",
    "df_sentiment = recuperer_sentiment_gdelt(\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    keyword=\"EUR/USD\",\n",
    "    language='eng',\n",
    "    chunk_days=30,\n",
    "    num_records=250\n",
    ")\n",
    "\n",
    "df_sentiment = df_sentiment.groupby('date')['sentiment'].mean()\n",
    "df_sentiment.index = pd.to_datetime(df_sentiment.index)\n",
    "\n",
    "print(df_sentiment.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b9df7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               close  rendement_log   vol_30j  rendement_log_10j  target  \\\n",
      "timestamp                                                                  \n",
      "2023-06-03  1.072559       0.000000  0.005411          -0.002793       0   \n",
      "2023-06-04  1.069745      -0.002626  0.004245          -0.002487       1   \n",
      "2023-06-05  1.071145       0.001308  0.004268          -0.001855       1   \n",
      "2023-06-06  1.069770      -0.001285  0.003061          -0.003139       1   \n",
      "2023-06-07  1.070710       0.000878  0.003072          -0.001455       1   \n",
      "2023-06-08  1.078237       0.007006  0.003367           0.006984       1   \n",
      "2023-06-09  1.076538      -0.001577  0.003343           0.003166       1   \n",
      "2023-06-10  1.076538       0.000000  0.003153           0.006702       1   \n",
      "2023-06-11  1.075061      -0.001373  0.003105          -0.000914       1   \n",
      "2023-06-12  1.076264       0.001119  0.003119           0.003449       1   \n",
      "\n",
      "                 mm7      mm21  boll_haut  boll_bas    rsi_14j  sentiment  \n",
      "timestamp                                                                  \n",
      "2023-06-03  1.072378  1.077441   1.088046  1.066056  33.354206   0.283725  \n",
      "2023-06-04  1.072017  1.076703   1.086497  1.065847  30.563519   0.000000  \n",
      "2023-06-05  1.072076  1.075932   1.084721  1.066084  34.671676  -0.116700  \n",
      "2023-06-06  1.071596  1.075134   1.083453  1.065949  37.900806  -0.440400  \n",
      "2023-06-07  1.071790  1.074511   1.083182  1.065540  41.690753  -0.057000  \n",
      "2023-06-08  1.072104  1.074546   1.082359  1.065950  58.684658  -0.256643  \n",
      "2023-06-09  1.072672  1.074268   1.081210  1.066517  54.928691   0.017650  \n",
      "2023-06-10  1.073240  1.073991   1.079931  1.067226  54.928691   0.000000  \n",
      "2023-06-11  1.074000  1.073649   1.078628  1.067928  53.972854   0.000000  \n",
      "2023-06-12  1.074731  1.073420   1.078506  1.067983  57.946325  -0.160760  \n"
     ]
    }
   ],
   "source": [
    "# --- Fusion des données de sentiment avec les données EUR/USD ---\n",
    "\n",
    "eur_usd = eur_usd.merge(df_sentiment.rename(\"sentiment\"), left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Remplir les NaN si aucun article\n",
    "eur_usd['sentiment'] = eur_usd['sentiment'].fillna(0)\n",
    "\n",
    "print(eur_usd.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fe5cba71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance expliquée par PCA : 97.37%\n",
      "Nombre de composantes PCA retenues : 5\n",
      "Forme finale du DataFrame : (844, 18)\n",
      "   timestamp     close  rendement_log   vol_30j  rendement_log_10j  target  \\\n",
      "0 2023-06-03  1.072559       0.000000  0.005411          -0.002793       0   \n",
      "1 2023-06-04  1.069745      -0.002626  0.004245          -0.002487       1   \n",
      "2 2023-06-05  1.071145       0.001308  0.004268          -0.001855       1   \n",
      "3 2023-06-06  1.069770      -0.001285  0.003061          -0.003139       1   \n",
      "4 2023-06-07  1.070710       0.000878  0.003072          -0.001455       1   \n",
      "5 2023-06-08  1.078237       0.007006  0.003367           0.006984       1   \n",
      "6 2023-06-09  1.076538      -0.001577  0.003343           0.003166       1   \n",
      "7 2023-06-10  1.076538       0.000000  0.003153           0.006702       1   \n",
      "8 2023-06-11  1.075061      -0.001373  0.003105          -0.000914       1   \n",
      "9 2023-06-12  1.076264       0.001119  0.003119           0.003449       1   \n",
      "\n",
      "        mm7      mm21  boll_haut  boll_bas    rsi_14j  sentiment  \\\n",
      "0  1.072378  1.077441   1.088046  1.066056  33.354206   0.283725   \n",
      "1  1.072017  1.076703   1.086497  1.065847  30.563519   0.000000   \n",
      "2  1.072076  1.075932   1.084721  1.066084  34.671676  -0.116700   \n",
      "3  1.071596  1.075134   1.083453  1.065949  37.900806  -0.440400   \n",
      "4  1.071790  1.074511   1.083182  1.065540  41.690753  -0.057000   \n",
      "5  1.072104  1.074546   1.082359  1.065950  58.684658  -0.256643   \n",
      "6  1.072672  1.074268   1.081210  1.066517  54.928691   0.017650   \n",
      "7  1.073240  1.073991   1.079931  1.067226  54.928691   0.000000   \n",
      "8  1.074000  1.073649   1.078628  1.067928  53.972854   0.000000   \n",
      "9  1.074731  1.073420   1.078506  1.067983  57.946325  -0.160760   \n",
      "\n",
      "   cluster_kmeans       PC1       PC2       PC3       PC4       PC5  \n",
      "0               0 -1.055490 -0.451792  2.199241  0.900503  0.591121  \n",
      "1               0 -1.250003 -0.988056  0.263746  0.782880 -0.063590  \n",
      "2               0 -1.220926 -0.438298 -0.454184  0.762158  0.786063  \n",
      "3               0 -1.379472 -0.854326 -2.653721  0.671161  0.060562  \n",
      "4               0 -1.367083 -0.392511 -0.537429 -0.337029  0.286892  \n",
      "5               2 -1.100105  1.295994 -1.734050 -0.199235  1.321251  \n",
      "6               2 -1.198175  0.163054 -0.072870 -0.204724 -0.595035  \n",
      "7               2 -1.198044  0.473528 -0.261585 -0.442885 -0.335375  \n",
      "8               0 -1.271032 -0.121296 -0.244354 -0.362574 -0.493825  \n",
      "9               2 -1.194966  0.487742 -1.201067 -0.202352 -0.022102  \n",
      "Index(['timestamp', 'close', 'rendement_log', 'vol_30j', 'rendement_log_10j',\n",
      "       'target', 'mm7', 'mm21', 'boll_haut', 'boll_bas', 'rsi_14j',\n",
      "       'sentiment', 'cluster_kmeans', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# --- Variables PCA & Clustering ---\n",
    "\n",
    "# Copie sécurisée\n",
    "df = eur_usd.copy().dropna().reset_index()\n",
    "\n",
    "# Sélection des features quantitatives \n",
    "# On exclut les colonnes non numériques ou non pertinentes\n",
    "features = df.drop(columns=['target'], errors='ignore').select_dtypes(include=[np.number])\n",
    "\n",
    "# Standardisation \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# PCA (réduction de dimension) \n",
    "# Garde assez de composantes pour expliquer 90 % de la variance\n",
    "pca = PCA(n_components=0.9, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Créer un DataFrame avec les composantes principales\n",
    "df_pca = pd.DataFrame(\n",
    "    X_pca,\n",
    "    columns=[f'PC{i+1}' for i in range(X_pca.shape[1])],\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# KMeans (clustering sur l’espace PCA) \n",
    "# Trouve des structures cachées dans les données\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "df['cluster_kmeans'] = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Fusion finale \n",
    "df_extended = pd.concat([df, df_pca], axis=1)\n",
    "\n",
    "# Résumé \n",
    "print(f\"Variance expliquée par PCA : {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"Nombre de composantes PCA retenues : {pca.n_components_}\")\n",
    "print(f\"Forme finale du DataFrame : {df_extended.shape}\")\n",
    "print(df_extended.head(10))\n",
    "print(df_extended.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fc82ae86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03\n",
      "2025-10-05\n",
      "Récupération variables macro de 2023-06-03 à 2025-10-05\n",
      "            inflation_eur  inflation_us  interest_rate_eur  interest_rate_us\n",
      "timestamp                                                                   \n",
      "2025-09-21       2.227488       2.93922                2.0              4.25\n",
      "2025-09-22       2.227488       2.93922                2.0              4.25\n",
      "2025-09-23       2.227488       2.93922                2.0              4.25\n",
      "2025-09-24       2.227488       2.93922                2.0              4.25\n",
      "2025-09-25       2.227488       2.93922                2.0              4.25\n",
      "2025-09-26       2.227488       2.93922                2.0              4.25\n",
      "2025-09-27       2.227488       2.93922                2.0              4.25\n",
      "2025-09-28       2.227488       2.93922                2.0              4.25\n",
      "2025-09-29       2.227488       2.93922                2.0              4.25\n",
      "2025-09-30       2.227488       2.93922                2.0              4.25\n",
      "(851, 4)\n"
     ]
    }
   ],
   "source": [
    "#----Variables macro----\n",
    "\n",
    "start_date = df_extended[\"timestamp\"].iloc[0].strftime(\"%Y-%m-%d\")\n",
    "print(start_date)\n",
    "end_date = df_extended[\"timestamp\"].iloc[-1].strftime(\"%Y-%m-%d\")\n",
    "print(end_date)\n",
    "\n",
    "print(f\"Récupération variables macro de {start_date} à {end_date}\")\n",
    "\n",
    "# Inflation\n",
    "infl_eur = taceconomics.getdata(f\"EUROSTAT/EI_CPHI_M_CP-HI00_NSA_HICP2015/EUZ?collapse=D&transform=growth_yoy&start_date={start_date}\")\n",
    "infl_us = taceconomics.getdata(f\"FRED/CPIAUCSL/USA?collapse=D&transform=growth_yoy&start_date={start_date}\")\n",
    "\n",
    " # Taux interets\n",
    "ti_eur = taceconomics.getdata(f\"ECB/FM_D_EUR_4F_KR_DFR_LEV/EUZ?collapse=D&collapse_mode=end_of_period&start_date={start_date}\")\n",
    "ti_us = taceconomics.getdata(f\"DS/USPRATE./WLD?collapse=D&start_date={start_date}\")\n",
    "\n",
    "# Dataframe macro\n",
    "df_macro = pd.DataFrame()\n",
    "df_macro[\"inflation_eur\"] = infl_eur\n",
    "df_macro[\"inflation_us\"] = infl_us\n",
    "df_macro[\"interest_rate_eur\"] = ti_eur\n",
    "df_macro[\"interest_rate_us\"] = ti_us\n",
    "\n",
    "df_macro = df_macro.ffill()\n",
    "\n",
    "print(df_macro.tail(10))\n",
    "print(df_macro.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b50f3313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           timestamp  eur_usd  taux_croissance  vol  rendement_10j  \\\n",
      "2025-09-21       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-22       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-23       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-24       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-25       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-26       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-27       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-28       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-29       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-30       NaT      NaN              NaN  NaN            NaN   \n",
      "\n",
      "            seuil_haut  seuil_bas  target  moyenne_mobile_7j  \\\n",
      "2025-09-21         NaN        NaN     NaN                NaN   \n",
      "2025-09-22         NaN        NaN     NaN                NaN   \n",
      "2025-09-23         NaN        NaN     NaN                NaN   \n",
      "2025-09-24         NaN        NaN     NaN                NaN   \n",
      "2025-09-25         NaN        NaN     NaN                NaN   \n",
      "2025-09-26         NaN        NaN     NaN                NaN   \n",
      "2025-09-27         NaN        NaN     NaN                NaN   \n",
      "2025-09-28         NaN        NaN     NaN                NaN   \n",
      "2025-09-29         NaN        NaN     NaN                NaN   \n",
      "2025-09-30         NaN        NaN     NaN                NaN   \n",
      "\n",
      "            moyenne_mobile_21j  ...  rsi_14j  PC1  PC2  PC3  PC4  \\\n",
      "2025-09-21                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-22                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-23                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-24                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-25                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-26                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-27                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-28                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-29                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-30                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "\n",
      "            cluster_kmeans  inflation_eur  inflation_us  interest_rate_eur  \\\n",
      "2025-09-21             NaN       2.227488           NaN                2.0   \n",
      "2025-09-22             NaN       2.227488           NaN                2.0   \n",
      "2025-09-23             NaN       2.227488           NaN                2.0   \n",
      "2025-09-24             NaN       2.227488           NaN                2.0   \n",
      "2025-09-25             NaN       2.227488           NaN                2.0   \n",
      "2025-09-26             NaN       2.227488           NaN                2.0   \n",
      "2025-09-27             NaN       2.227488           NaN                2.0   \n",
      "2025-09-28             NaN       2.227488           NaN                2.0   \n",
      "2025-09-29             NaN       2.227488           NaN                2.0   \n",
      "2025-09-30             NaN       2.227488           NaN                2.0   \n",
      "\n",
      "            interest_rate_us  \n",
      "2025-09-21              4.25  \n",
      "2025-09-22              4.25  \n",
      "2025-09-23              4.25  \n",
      "2025-09-24              4.25  \n",
      "2025-09-25              4.25  \n",
      "2025-09-26              4.25  \n",
      "2025-09-27              4.25  \n",
      "2025-09-28              4.25  \n",
      "2025-09-29              4.25  \n",
      "2025-09-30              4.25  \n",
      "\n",
      "[10 rows x 23 columns]\n",
      "Index(['timestamp', 'eur_usd', 'taux_croissance', 'vol', 'rendement_10j',\n",
      "       'seuil_haut', 'seuil_bas', 'target', 'moyenne_mobile_7j',\n",
      "       'moyenne_mobile_21j', 'bollinger_moyenne', 'bollinger_haut',\n",
      "       'bollinger_bas', 'rsi_14j', 'PC1', 'PC2', 'PC3', 'PC4',\n",
      "       'cluster_kmeans', 'inflation_eur', 'inflation_us', 'interest_rate_eur',\n",
      "       'interest_rate_us'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.concat([df_extended, df_macro], axis=1)\n",
    "print(df_final.tail(10))\n",
    "print(df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac8013b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Variable cible correctement décalée de 2 semaines.\n",
      "Shape finale : (954, 18), target positive rate = 0.059\n",
      "Chargement des données...\n",
      "Tailles -> train: (667, 18), val: (143, 18), test: (144, 18)\n"
     ]
    },
    {
     "ename": "DTypePromotionError",
     "evalue": "The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int32DType'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDTypePromotionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 118\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Imputation (median) - fit uniquement sur train\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m    117\u001b[39m imputer = SimpleImputer(strategy=\u001b[33m'\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[43mimputer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m X_train_imp = pd.DataFrame(imputer.transform(X_train[num_cols]), index=X_train.index, columns=num_cols)\n\u001b[32m    121\u001b[39m X_val_imp   = pd.DataFrame(imputer.transform(X_val[num_cols]), index=X_val.index, columns=num_cols)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python D.Liron/fx-shock-prediction/venv/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python D.Liron/fx-shock-prediction/venv/lib/python3.12/site-packages/sklearn/impute/_base.py:452\u001b[39m, in \u001b[36mSimpleImputer.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    436\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the imputer on `X`.\u001b[39;00m\n\u001b[32m    437\u001b[39m \n\u001b[32m    438\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[39;00m\n\u001b[32m    455\u001b[39m     \u001b[38;5;66;03m# otherwise\u001b[39;00m\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python D.Liron/fx-shock-prediction/venv/lib/python3.12/site-packages/sklearn/impute/_base.py:360\u001b[39m, in \u001b[36mSimpleImputer._validate_input\u001b[39m\u001b[34m(self, X, in_fit)\u001b[39m\n\u001b[32m    357\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[32m    371\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcould not convert\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python D.Liron/fx-shock-prediction/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python D.Liron/fx-shock-prediction/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:929\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    925\u001b[39m pandas_requires_conversion = \u001b[38;5;28many\u001b[39m(\n\u001b[32m    926\u001b[39m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[32m    927\u001b[39m )\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np.dtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     dtype_orig = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdtypes_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[32m    931\u001b[39m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[32m    932\u001b[39m     dtype_orig = \u001b[38;5;28mobject\u001b[39m\n",
      "\u001b[31mDTypePromotionError\u001b[39m: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int32DType'>)"
     ]
    }
   ],
   "source": [
    "# MODELISATION\n",
    "\n",
    "# xgb_single_model_timeseries.py\n",
    "\"\"\"\n",
    "XGBoost focalisé pour prédiction d'un choc à 2 semaines sur EUR/USD.\n",
    "Sorties :\n",
    "- AUC, Gini, courbe ROC (test),\n",
    "- seuil optimal (Youden) et matrice de confusion + métriques associées,\n",
    "- modèle final enregistré (joblib).\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------\n",
    "# Config \n",
    "# -----------------------\n",
    "RANDOM_STATE = 42\n",
    "# Fractions pour split contigu (doivent sommer à 1.0)\n",
    "TRAIN_FRAC = 0.70\n",
    "VAL_FRAC   = 0.15\n",
    "TEST_FRAC  = 0.15\n",
    "\n",
    "# TimeSeries CV splits (pour GridSearch)\n",
    "TS_SPLITS = 5\n",
    "\n",
    "# Grid search params \n",
    "XGB_PARAM_GRID = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Metric d'optimisation pour la recherche d'hyperparamètres (ici F1, tu peux changer en 'roc_auc')\n",
    "GRID_SCORING = 'f1'\n",
    "\n",
    "# Early stopping rounds pour re-entrainement final\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "# Fichiers de sortie\n",
    "MODEL_OUTPATH = \"xgb_final_model.joblib\"\n",
    "IMPUTER_OUTPATH = \"imputer.joblib\"\n",
    "\n",
    "# ===============================\n",
    "# Décalage de la variable cible pour prédire le choc à 2 semaines\n",
    "# ===============================\n",
    "\n",
    "SHIFT_DAYS = 10  # environ 2 semaines ouvrées\n",
    "\n",
    "# On crée une copie pour éviter les erreurs d’alignement\n",
    "df_final = df_extended.copy()\n",
    "\n",
    "# Décaler la target vers le passé : les features du jour t servent à prédire le choc à t+10\n",
    "df_final[\"target_future\"] = df_final[\"target\"].shift(-SHIFT_DAYS)\n",
    "\n",
    "# Supprimer les lignes où la cible future est manquante (en fin de série)\n",
    "df_final = df_final.dropna(subset=[\"target_future\"])\n",
    "\n",
    "# Définir X (features actuelles) et y (choc futur)\n",
    "X = df_final.drop(columns=[\"target\", \"target_future\"])  # on garde uniquement les variables explicatives\n",
    "y = df_final[\"target_future\"].astype(int)\n",
    "\n",
    "print(\"✅ Variable cible correctement décalée de 2 semaines.\")\n",
    "print(f\"Shape finale : {X.shape}, target positive rate = {y.mean():.3f}\")\n",
    "\n",
    "# -----------------------\n",
    "# Chargement & checks\n",
    "# -----------------------\n",
    "print(\"Chargement des données...\")\n",
    "\n",
    "df = df_final.copy().dropna().sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "# Features & target\n",
    "X = df.drop(columns=[\"target\", \"target_future\"])\n",
    "y = df[\"target_future\"].astype(int)\n",
    "\n",
    "# On suppose tout numérique — sinon adapter types/catégoriques\n",
    "num_cols = X.columns.tolist()\n",
    "\n",
    "# -----------------------\n",
    "# Split contigu (train / val / test)\n",
    "# -----------------------\n",
    "n = len(df)\n",
    "if not abs(TRAIN_FRAC + VAL_FRAC + TEST_FRAC - 1.0) < 1e-8:\n",
    "    raise ValueError(\"TRAIN_FRAC + VAL_FRAC + TEST_FRAC doit être égal à 1.0\")\n",
    "\n",
    "train_end = int(n * TRAIN_FRAC)\n",
    "val_end = train_end + int(n * VAL_FRAC)\n",
    "\n",
    "X_train = X.iloc[:train_end].copy()\n",
    "y_train = y.iloc[:train_end].copy()\n",
    "\n",
    "X_val = X.iloc[train_end:val_end].copy()\n",
    "y_val = y.iloc[train_end:val_end].copy()\n",
    "\n",
    "X_test = X.iloc[val_end:].copy()\n",
    "y_test = y.iloc[val_end:].copy()\n",
    "\n",
    "print(f\"Tailles -> train: {X_train.shape}, val: {X_val.shape}, test: {X_test.shape}\")\n",
    "\n",
    "# -----------------------\n",
    "# Imputation (median) - fit uniquement sur train\n",
    "# -----------------------\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(X_train[num_cols])\n",
    "\n",
    "X_train_imp = pd.DataFrame(imputer.transform(X_train[num_cols]), index=X_train.index, columns=num_cols)\n",
    "X_val_imp   = pd.DataFrame(imputer.transform(X_val[num_cols]), index=X_val.index, columns=num_cols)\n",
    "X_test_imp  = pd.DataFrame(imputer.transform(X_test[num_cols]), index=X_test.index, columns=num_cols)\n",
    "\n",
    "# Save imputer for reproducibility / production\n",
    "joblib.dump(imputer, IMPUTER_OUTPATH)\n",
    "print(f\"Imputer sauvegardé -> {IMPUTER_OUTPATH}\")\n",
    "\n",
    "# -----------------------\n",
    "# GridSearchCV (TimeSeriesSplit) pour chercher les meilleurs hyperparams\n",
    "# -----------------------\n",
    "print(\"\\nLancement GridSearchCV (TimeSeriesSplit) sur XGBoost...\")\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=TS_SPLITS)\n",
    "\n",
    "# GridSearchCV sur les features imputées (pas de scaling nécessaire pour XGBoost)\n",
    "gscv = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=XGB_PARAM_GRID,\n",
    "    scoring=GRID_SCORING,\n",
    "    cv=tscv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "gscv.fit(X_train_imp, y_train)\n",
    "print(\"Meilleurs paramètres trouvés (GridSearchCV):\")\n",
    "print(gscv.best_params_)\n",
    "print(f\"Best CV {GRID_SCORING}: {gscv.best_score_:.4f}\")\n",
    "\n",
    "# -----------------------\n",
    "# Ré-entraînement final avec early stopping sur l'échantillon de validation\n",
    "# -----------------------\n",
    "best_params = gscv.best_params_.copy()\n",
    "\n",
    "# Conserver paramètres choisis et activer early stopping via eval_set\n",
    "xgb_final = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE,\n",
    "                          n_jobs=-1, **best_params)\n",
    "\n",
    "print(\"\\nRé-entrainement final avec early stopping sur validation (eval_set)...\")\n",
    "xgb_final.fit(\n",
    "    X_train_imp, y_train,\n",
    "    eval_set=[(X_val_imp, y_val)],\n",
    "    early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Sauvegarde modèle\n",
    "joblib.dump(xgb_final, MODEL_OUTPATH)\n",
    "print(f\"Modèle final sauvegardé -> {MODEL_OUTPATH}\")\n",
    "\n",
    "# -----------------------\n",
    "# Prédiction out-of-sample (test) + évaluation\n",
    "# -----------------------\n",
    "print(\"\\nÉvaluation out-of-sample (test)...\")\n",
    "probs_test = xgb_final.predict_proba(X_test_imp)[:, 1]\n",
    "auc = roc_auc_score(y_test, probs_test)\n",
    "gini = 2*auc - 1\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs_test)\n",
    "\n",
    "# Seuil optimal - Youden (TPR - FPR maximisé)\n",
    "youden_idx = np.argmax(tpr - fpr)\n",
    "opt_threshold_youden = thresholds[youden_idx]\n",
    "\n",
    "# Seuil optimisant F1 (pour info)\n",
    "f1_scores = [f1_score(y_test, (probs_test >= t).astype(int), zero_division=0) for t in thresholds]\n",
    "opt_threshold_f1 = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# Choix du seuil final : tu peux choisir Youden ou F1 ; ici on utilise Youden tout en reportant F1-opt\n",
    "threshold = opt_threshold_youden\n",
    "\n",
    "preds = (probs_test >= threshold).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "\n",
    "precision = precision_score(y_test, preds, zero_division=0)\n",
    "recall = recall_score(y_test, preds, zero_division=0)            # sensitivity\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "f1 = f1_score(y_test, preds, zero_division=0)\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "bal_acc = balanced_accuracy_score(y_test, preds)\n",
    "\n",
    "# -----------------------\n",
    "# Affichage résultats (out-of-sample uniquement)\n",
    "# -----------------------\n",
    "print(f\"\\nRésultats (test) - XGBoost\")\n",
    "print(f\"AUC (ROC): {auc:.4f}\")\n",
    "print(f\"Gini: {gini:.4f}\")\n",
    "print(f\"Seuil Youden: {opt_threshold_youden:.4f} | Seuil F1-opt: {opt_threshold_f1:.4f}\")\n",
    "print(\"Matrice de confusion (tn, fp, fn, tp):\", (int(tn), int(fp), int(fn), int(tp)))\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"F1: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Balanced Accuracy: {bal_acc:.4f}\")\n",
    "\n",
    "# -----------------------\n",
    "# Tracer ROC (test)\n",
    "# -----------------------\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.plot(fpr, tpr, label=f\"XGBoost (AUC={auc:.3f})\")\n",
    "plt.plot([0,1],[0,1], linestyle='--', alpha=0.6)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC - Out-of-sample (test)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------\n",
    "# Tracer Matrice de confusion (test)\n",
    "# -----------------------\n",
    "cm = np.array([[tn, fp],[fn, tp]])\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f\"Matrice de confusion (test) - seuil={threshold:.4f}\")\n",
    "plt.ylabel(\"Vraie classe\")\n",
    "plt.xlabel(\"Classe prédite\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------\n",
    "# Importance des features (optionnel, utile pour interprétation)\n",
    "# -----------------------\n",
    "try:\n",
    "    imp = pd.Series(xgb_final.feature_importances_, index=num_cols).sort_values(ascending=False)\n",
    "    print(\"\\nTop 10 features par importance (XGBoost):\")\n",
    "    print(imp.head(10))\n",
    "    plt.figure(figsize=(6,4))\n",
    "    imp.head(15).plot(kind='bar')\n",
    "    plt.title(\"Feature importances (XGBoost)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -----------------------\n",
    "# Résumé final en dictionary (pratique pour reporting programmatique)\n",
    "# -----------------------\n",
    "report = {\n",
    "    'auc': float(auc),\n",
    "    'gini': float(gini),\n",
    "    'threshold_youden': float(opt_threshold_youden),\n",
    "    'threshold_f1': float(opt_threshold_f1),\n",
    "    'threshold_used': float(threshold),\n",
    "    'confusion': {'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)},\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'specificity': float(specificity),\n",
    "    'f1': float(f1),\n",
    "    'accuracy': float(accuracy),\n",
    "    'balanced_accuracy': float(bal_acc),\n",
    "    'best_params': best_params,\n",
    "    'trained_at': datetime.utcnow().isoformat() + 'Z'\n",
    "}\n",
    "\n",
    "# Enregistrer le reporting si souhaité\n",
    "pd.Series(report).to_json(\"xgb_report_test.json\")\n",
    "print(\"\\nReport JSON sauvegardé -> xgb_report_test.json\")\n",
    "print(\"\\nTerminé. Seuls les résultats out-of-sample (test) ont été affichés.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
