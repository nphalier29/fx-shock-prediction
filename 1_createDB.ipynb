{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6391b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import taceconomics\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "385b638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APIKEY\n",
    "taceconomics.api_key = \"sk_o24BhJRqVpIvxVSXX3yiKRGgpDEejmyJ8pfLFX2q22s\"  \n",
    "start_date = '2023-01-01'\n",
    "end_date = datetime.today().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "18cfe72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             eur_usd\n",
      "timestamp           \n",
      "2023-01-01  1.070452\n",
      "2023-01-02  1.067737\n",
      "2023-01-03  1.054652\n",
      "2023-01-04  1.060839\n",
      "2023-01-05  1.052192\n",
      "2023-01-06  1.066382\n",
      "2023-01-07  1.066382\n",
      "2023-01-08  1.065598\n",
      "2023-01-09  1.073445\n",
      "2023-01-10  1.073860\n",
      "(994, 1)\n"
     ]
    }
   ],
   "source": [
    "# --- Importation des données ---\n",
    "\n",
    "# Taux de change EUR/USD\n",
    "usd_eur = taceconomics.getdata(f\"EXR/EUR/WLD?start_date={start_date}\")\n",
    "usd_eur.columns = [\"usd_eur\"]\n",
    "usd_eur = usd_eur.dropna()\n",
    "eur_usd = 1 / usd_eur  # Inversion pour avoir EUR/USD\n",
    "eur_usd.columns = [\"eur_usd\"]\n",
    "eur_usd.index = pd.to_datetime(eur_usd.index)\n",
    "\n",
    "print(eur_usd.head(10))\n",
    "print(eur_usd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "60ddabcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             eur_usd  taux_croissance  vol  rendement_10j\n",
      "timestamp                                                \n",
      "2023-01-01  1.070452              NaN  NaN            NaN\n",
      "2023-01-02  1.067737        -0.253588  NaN            NaN\n",
      "2023-01-03  1.054652        -1.225506  NaN            NaN\n",
      "2023-01-04  1.060839         0.586644  NaN            NaN\n",
      "2023-01-05  1.052192        -0.815133  NaN            NaN\n",
      "2023-01-06  1.066382         1.348654  NaN            NaN\n",
      "2023-01-07  1.066382         0.000000  NaN            NaN\n",
      "2023-01-08  1.065598        -0.073526  NaN            NaN\n",
      "2023-01-09  1.073445         0.736383  NaN            NaN\n",
      "2023-01-10  1.073860         0.038659  NaN            NaN\n",
      "(994, 4)\n"
     ]
    }
   ],
   "source": [
    "# --- Indicateurs quantitatifs ---\n",
    "\n",
    "# Taux de croissance\n",
    "eur_usd[\"taux_croissance\"] = eur_usd[\"eur_usd\"].pct_change() * 100\n",
    "\n",
    "# Vol\n",
    "eur_usd[\"vol\"] = eur_usd[\"taux_croissance\"].rolling(window=30).std()\n",
    "\n",
    "# rendement à 10j (2 semaines en jours ouvrés)\n",
    "eur_usd[\"rendement_10j\"] = eur_usd[\"eur_usd\"].pct_change(periods=10) * 100\n",
    "\n",
    "print(eur_usd.head(10))\n",
    "print(eur_usd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "868dedf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            taux_croissance  vol  target\n",
      "timestamp                               \n",
      "2023-01-01              NaN  NaN       0\n",
      "2023-01-02        -0.253588  NaN       0\n",
      "2023-01-03        -1.225506  NaN       0\n",
      "2023-01-04         0.586644  NaN       0\n",
      "2023-01-05        -0.815133  NaN       0\n",
      "2023-01-06         1.348654  NaN       0\n",
      "2023-01-07         0.000000  NaN       0\n",
      "2023-01-08        -0.073526  NaN       0\n",
      "2023-01-09         0.736383  NaN       0\n",
      "2023-01-10         0.038659  NaN       0\n",
      "(994, 7)\n"
     ]
    }
   ],
   "source": [
    "# --- Cible --- \n",
    "\n",
    "# Seuil de choc (±2σ)\n",
    "eur_usd[\"seuil_haut\"] = 2 * eur_usd[\"vol\"]\n",
    "eur_usd[\"seuil_bas\"] = -2 * eur_usd[\"vol\"]\n",
    "\n",
    "# Variable cible : 1 si choc (hausse ou baisse), 0 sinon\n",
    "eur_usd[\"target\"] = ((eur_usd[\"taux_croissance\"] >= eur_usd[\"seuil_haut\"]) | (eur_usd[\"taux_croissance\"] <= eur_usd[\"seuil_bas\"])).astype(int)\n",
    "\n",
    "print(eur_usd[[\"taux_croissance\", \"vol\", \"target\"]].head(10))\n",
    "print(eur_usd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fbf84b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             eur_usd  taux_croissance  vol  rendement_10j  seuil_haut  \\\n",
      "timestamp                                                               \n",
      "2023-01-01  1.070452              NaN  NaN            NaN         NaN   \n",
      "2023-01-02  1.067737        -0.253588  NaN            NaN         NaN   \n",
      "2023-01-03  1.054652        -1.225506  NaN            NaN         NaN   \n",
      "2023-01-04  1.060839         0.586644  NaN            NaN         NaN   \n",
      "2023-01-05  1.052192        -0.815133  NaN            NaN         NaN   \n",
      "2023-01-06  1.066382         1.348654  NaN            NaN         NaN   \n",
      "2023-01-07  1.066382         0.000000  NaN            NaN         NaN   \n",
      "2023-01-08  1.065598        -0.073526  NaN            NaN         NaN   \n",
      "2023-01-09  1.073445         0.736383  NaN            NaN         NaN   \n",
      "2023-01-10  1.073860         0.038659  NaN            NaN         NaN   \n",
      "\n",
      "            seuil_bas  target  moyenne_mobile_7j  moyenne_mobile_21j  \\\n",
      "timestamp                                                              \n",
      "2023-01-01        NaN       0                NaN                 NaN   \n",
      "2023-01-02        NaN       0                NaN                 NaN   \n",
      "2023-01-03        NaN       0                NaN                 NaN   \n",
      "2023-01-04        NaN       0                NaN                 NaN   \n",
      "2023-01-05        NaN       0                NaN                 NaN   \n",
      "2023-01-06        NaN       0                NaN                 NaN   \n",
      "2023-01-07        NaN       0           1.062662                 NaN   \n",
      "2023-01-08        NaN       0           1.061969                 NaN   \n",
      "2023-01-09        NaN       0           1.062784                 NaN   \n",
      "2023-01-10        NaN       0           1.065528                 NaN   \n",
      "\n",
      "            bollinger_moyenne  bollinger_haut  bollinger_bas  \n",
      "timestamp                                                     \n",
      "2023-01-01                NaN             NaN            NaN  \n",
      "2023-01-02                NaN             NaN            NaN  \n",
      "2023-01-03                NaN             NaN            NaN  \n",
      "2023-01-04                NaN             NaN            NaN  \n",
      "2023-01-05                NaN             NaN            NaN  \n",
      "2023-01-06                NaN             NaN            NaN  \n",
      "2023-01-07                NaN             NaN            NaN  \n",
      "2023-01-08                NaN             NaN            NaN  \n",
      "2023-01-09                NaN             NaN            NaN  \n",
      "2023-01-10                NaN             NaN            NaN  \n",
      "(994, 12)\n",
      "(994, 13)\n",
      "             eur_usd  taux_croissance       vol  rendement_10j  seuil_haut  \\\n",
      "timestamp                                                                    \n",
      "2025-09-25  1.166222        -0.697400  0.374943      -0.860672    0.749886   \n",
      "2025-09-26  1.170254         0.345810  0.379859      -1.418934    0.759717   \n",
      "2025-09-27  1.170254         0.000000  0.376338      -1.041526    0.752677   \n",
      "2025-09-28  1.170682         0.036525  0.376349      -0.703463    0.752698   \n",
      "2025-09-29  1.173117         0.207994  0.378129      -0.122591    0.756258   \n",
      "2025-09-30  1.173530         0.035206  0.378099      -0.087428    0.756199   \n",
      "2025-10-01  1.173351        -0.015254  0.376207      -0.025814    0.752413   \n",
      "2025-10-03  1.174117         0.065281  0.352563      -0.502405    0.705126   \n",
      "2025-10-04  1.174117         0.000000  0.350815      -0.615707    0.701629   \n",
      "2025-10-05  1.171173        -0.250748  0.354277      -0.275811    0.708553   \n",
      "\n",
      "            seuil_bas  target  moyenne_mobile_7j  moyenne_mobile_21j  \\\n",
      "timestamp                                                              \n",
      "2025-09-25  -0.749886       0           1.174977            1.175048   \n",
      "2025-09-26  -0.759717       0           1.174362            1.174956   \n",
      "2025-09-27  -0.752677       0           1.173747            1.174864   \n",
      "2025-09-28  -0.752698       0           1.173323            1.174836   \n",
      "2025-09-29  -0.756258       0           1.172333            1.174657   \n",
      "2025-09-30  -0.756199       0           1.171210            1.174814   \n",
      "2025-10-01  -0.752413       0           1.171059            1.174959   \n",
      "2025-10-03  -0.705126       0           1.172186            1.174993   \n",
      "2025-10-04  -0.701629       0           1.172738            1.175019   \n",
      "2025-10-05  -0.708553       0           1.172869            1.174905   \n",
      "\n",
      "            bollinger_moyenne  bollinger_haut  bollinger_bas    rsi_14j  \n",
      "timestamp                                                                \n",
      "2025-09-25           1.175191        1.184916       1.165467  43.084469  \n",
      "2025-09-26           1.175095        1.184982       1.165207  47.036885  \n",
      "2025-09-27           1.175044        1.185024       1.165063  47.036885  \n",
      "2025-09-28           1.174734        1.184858       1.164610  48.277969  \n",
      "2025-09-29           1.174878        1.184812       1.164945  47.009950  \n",
      "2025-09-30           1.175040        1.184763       1.165317  34.460598  \n",
      "2025-10-01           1.175037        1.184762       1.165312  38.272434  \n",
      "2025-10-03           1.175064        1.184775       1.165354  43.340753  \n",
      "2025-10-04           1.175092        1.184788       1.165396  49.313957  \n",
      "2025-10-05           1.175021        1.184814       1.165228  45.166598  \n"
     ]
    }
   ],
   "source": [
    "# --- Indicateurs chartistes pour visualisation ---\n",
    "\n",
    "# Moyennes mobiles\n",
    "eur_usd[\"moyenne_mobile_7j\"] = eur_usd[\"eur_usd\"].rolling(window=7).mean()\n",
    "eur_usd[\"moyenne_mobile_21j\"] = eur_usd[\"eur_usd\"].rolling(window=21).mean()\n",
    "\n",
    "# Bandes de Bollinger\n",
    "eur_usd[\"bollinger_moyenne\"] = eur_usd[\"eur_usd\"].rolling(window=20).mean()\n",
    "eur_usd[\"bollinger_haut\"] = eur_usd[\"bollinger_moyenne\"] + 2 * eur_usd[\"eur_usd\"].rolling(window=20).std()\n",
    "eur_usd[\"bollinger_bas\"] = eur_usd[\"bollinger_moyenne\"] - 2 * eur_usd[\"eur_usd\"].rolling(window=20).std()\n",
    "\n",
    "print(eur_usd.head(10))\n",
    "print(eur_usd.shape)\n",
    "\n",
    "\n",
    "# RSI (Relative Strength Index)\n",
    "def calculer_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    gains = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    pertes = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gains / pertes\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "eur_usd[\"rsi_14j\"] = calculer_rsi(eur_usd[\"eur_usd\"])\n",
    "\n",
    "print(eur_usd.shape)\n",
    "print(eur_usd.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4032854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Indicateur de sentiment ---\n",
    "from gdeltdoc import GdeltDoc, Filters\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Télécharger VADER\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def recuperer_tous_articles_gdelt(start_date, end_date, keyword=\"EUR/USD\", language='eng', chunk_days=30):\n",
    "    \"\"\"\n",
    "    Récupère tous les articles GDELT entre start_date et end_date\n",
    "    en faisant des requêtes par tranches de `chunk_days` jours.\n",
    "    \"\"\"\n",
    "    # Initialiser VADER\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Convertir les dates en objets datetime\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    # Initialiser GdeltDoc\n",
    "    gd = GdeltDoc()\n",
    "    all_articles = []\n",
    "\n",
    "    # Parcourir par tranches de `chunk_days` jours\n",
    "    current_start = start\n",
    "    while current_start < end:\n",
    "        current_end = min(current_start + timedelta(days=chunk_days), end)\n",
    "\n",
    "        print(f\"Récupération des articles entre {current_start.date()} et {current_end.date()}\")\n",
    "\n",
    "        # Créer les filtres pour la tranche actuelle\n",
    "        f = Filters(\n",
    "            start_date=current_start.strftime(\"%Y-%m-%d\"),\n",
    "            end_date=current_end.strftime(\"%Y-%m-%d\"),\n",
    "            num_records=250,\n",
    "            keyword=keyword,\n",
    "            language=language\n",
    "        )\n",
    "\n",
    "        # Récupérer les articles\n",
    "        articles_df = gd.article_search(f)\n",
    "\n",
    "        # Vérifier si le DataFrame n'est pas vide\n",
    "        if not articles_df.empty:\n",
    "            all_articles.append(articles_df)\n",
    "\n",
    "        # Passer à la tranche suivante\n",
    "        current_start = current_end + timedelta(days=1)\n",
    "\n",
    "    # Concaténer tous les DataFrames\n",
    "    if not all_articles:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.concat(all_articles, ignore_index=True)\n",
    "\n",
    "    # Nettoyer les colonnes utiles et convertir la date\n",
    "    def convertir_date_gdelt(date_str):\n",
    "        # Le format est AAAAMMJJTHHMMSSZ, on extrait AAAAMMJJ\n",
    "        return datetime.strptime(date_str.split('T')[0], \"%Y%m%d\").date()\n",
    "\n",
    "    df['date'] = df['seendate'].apply(convertir_date_gdelt)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Calculer le sentiment pour chaque article\n",
    "    def calculer_sentiment(texte):\n",
    "        if isinstance(texte, str):\n",
    "            return sid.polarity_scores(texte)['compound']\n",
    "        return 0\n",
    "\n",
    "    df['sentiment'] = df['title'].apply(calculer_sentiment)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fe5cba71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(964, 19)\n",
      "   timestamp   eur_usd  taux_croissance       vol  rendement_10j  seuil_haut  \\\n",
      "0 2023-02-01  1.101249         1.394181  0.535094       1.361144    1.070189   \n",
      "1 2023-02-02  1.090477        -0.978158  0.566828       0.299881    1.133656   \n",
      "2 2023-02-03  1.082778        -0.705972  0.532736      -0.551134    1.065472   \n",
      "3 2023-02-04  1.082778         0.000000  0.524549      -0.859726    1.049097   \n",
      "4 2023-02-05  1.079051        -0.344217  0.503796      -0.954313    1.007592   \n",
      "5 2023-02-07  1.072938        -0.566511  0.457390      -1.316495    0.914780   \n",
      "6 2023-02-08  1.071639        -0.121095  0.458120      -1.435996    0.916239   \n",
      "7 2023-02-09  1.074056         0.225552  0.459325      -1.200795    0.918650   \n",
      "8 2023-02-10  1.069919        -0.385171  0.445048      -1.385010    0.890097   \n",
      "9 2023-02-11  1.070148         0.021403  0.444995      -1.469313    0.889989   \n",
      "\n",
      "   seuil_bas  target  moyenne_mobile_7j  moyenne_mobile_21j  \\\n",
      "0  -1.070189       1           1.089052            1.086332   \n",
      "1  -1.133656       0           1.089199            1.087001   \n",
      "2  -1.065472       0           1.088560            1.086849   \n",
      "3  -1.049097       0           1.087921            1.086754   \n",
      "4  -1.007592       0           1.086769            1.086481   \n",
      "5  -0.914780       0           1.085054            1.086029   \n",
      "6  -0.916239       0           1.082987            1.085489   \n",
      "7  -0.918650       0           1.079103            1.085267   \n",
      "8  -0.890097       0           1.076166            1.084620   \n",
      "9  -0.889989       0           1.074362            1.083786   \n",
      "\n",
      "   bollinger_moyenne  bollinger_haut  bollinger_bas    rsi_14j       PC1  \\\n",
      "0           1.086827        1.095692       1.077963  78.506598  0.317610   \n",
      "1           1.087052        1.096054       1.078051  57.662471  0.192901   \n",
      "2           1.086953        1.096103       1.077802  45.020829 -0.077953   \n",
      "3           1.086853        1.096146       1.077559  44.949100 -0.116926   \n",
      "4           1.086684        1.096427       1.076941  42.796818 -0.254710   \n",
      "5           1.086182        1.097617       1.074747  37.428372 -0.531791   \n",
      "6           1.085827        1.098598       1.073057  34.838317 -0.595804   \n",
      "7           1.085355        1.099145       1.071564  33.698272 -0.620080   \n",
      "8           1.084468        1.099827       1.069108  32.860130 -0.795891   \n",
      "9           1.083589        1.100129       1.067048  34.451672 -0.847293   \n",
      "\n",
      "        PC2       PC3       PC4  cluster_kmeans  \n",
      "0  2.456596  2.956575  4.199956               2  \n",
      "1  2.262386 -1.016143 -0.692909               2  \n",
      "2  1.662849 -1.611783 -0.413473               2  \n",
      "3  1.627019 -1.121246 -0.075201               2  \n",
      "4  1.272299 -1.509456 -0.211303               2  \n",
      "5  0.542814 -1.977113 -0.256696               0  \n",
      "6  0.607560 -1.740285 -0.029132               0  \n",
      "7  0.714651 -1.339321  0.113123               0  \n",
      "8  0.418244 -1.985840 -0.143061               0  \n",
      "9  0.499506 -1.602357  0.030828               0  \n"
     ]
    }
   ],
   "source": [
    "# --- Variables PCA ---\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df = eur_usd.copy().dropna().reset_index()\n",
    "\n",
    "X = df.select_dtypes(include=['float64','int64'])  # uniquement variables numériques\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=0.9)  # garder assez de composantes pour expliquer 90% de la variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Créer un DataFrame avec les nouvelles variables PCA\n",
    "df_pca = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])\n",
    "\n",
    "df_extended = pd.concat([df, df_pca], axis=1)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)  # exemple avec 3 clusters\n",
    "df_extended['cluster_kmeans'] = kmeans.fit_predict(X_pca)\n",
    "\n",
    "\n",
    "print(df_extended.shape)\n",
    "print(df_extended.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fc82ae86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01\n",
      "2025-10-05\n",
      "            inflation_eur  inflation_us  interest_rate_eur  interest_rate_us\n",
      "timestamp                                                                   \n",
      "2025-09-21       2.227488           NaN                2.0              4.25\n",
      "2025-09-22       2.227488           NaN                2.0              4.25\n",
      "2025-09-23       2.227488           NaN                2.0              4.25\n",
      "2025-09-24       2.227488           NaN                2.0              4.25\n",
      "2025-09-25       2.227488           NaN                2.0              4.25\n",
      "2025-09-26       2.227488           NaN                2.0              4.25\n",
      "2025-09-27       2.227488           NaN                2.0              4.25\n",
      "2025-09-28       2.227488           NaN                2.0              4.25\n",
      "2025-09-29       2.227488           NaN                2.0              4.25\n",
      "2025-09-30       2.227488           NaN                2.0              4.25\n",
      "(973, 4)\n"
     ]
    }
   ],
   "source": [
    "#----Variables macro----\n",
    "\n",
    "# Inflation\n",
    "\n",
    "start_date = df_extended[\"timestamp\"].iloc[0].strftime(\"%Y-%m-%d\")\n",
    "print(start_date)\n",
    "end_date = df_extended[\"timestamp\"].iloc[-1].strftime(\"%Y-%m-%d\")\n",
    "print(end_date)\n",
    "\n",
    "infl_eur = taceconomics.getdata(f\"EUROSTAT/EI_CPHI_M_CP-HI00_NSA_HICP2015/EUZ?collapse=D&transform=growth_yoy&start_date={start_date}\")\n",
    "infl_us = taceconomics.getdata(f\"FRED/CPIAUCSL/USA?collapse=D&transform=growth_yoy&start_date={start_date}\")\n",
    "\n",
    "# infl_eur = taceconomics.getdata(f\"IFS/PCPIHA_IX_M/EUZ?start_date={start_date}\")\n",
    "# infl_us = taceconomics.getdata(f\"IFS/PCPI_IX_M/USA?start_date={start_date}\")\n",
    "\n",
    "df_macro = pd.DataFrame()\n",
    "df_macro[\"inflation_eur\"] = infl_eur\n",
    "df_macro[\"inflation_us\"] = infl_us\n",
    "\n",
    " # Taux interets\n",
    "\n",
    "ti_eur = taceconomics.getdata(f\"ECB/FM_D_EUR_4F_KR_DFR_LEV/EUZ?collapse=D&collapse_mode=end_of_period&start_date={start_date}\")\n",
    "ti_us = taceconomics.getdata(f\"DS/USPRATE./WLD?collapse=D&start_date={start_date}\")\n",
    "\n",
    "# ti_eur = taceconomics.getdata(f\"IFS/FPOLM_PA_M/EUZ?start_date={start_date}\")\n",
    "# ti_us = taceconomics.getdata(f\"IFS/FPOLM_PA_M/USA?start_date={start_date}\")\n",
    "\n",
    "df_macro[\"interest_rate_eur\"] = ti_eur\n",
    "df_macro[\"interest_rate_us\"] = ti_us\n",
    "\n",
    "print(df_macro.tail(10))\n",
    "print(df_macro.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b50f3313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           timestamp  eur_usd  taux_croissance  vol  rendement_10j  \\\n",
      "2025-09-21       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-22       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-23       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-24       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-25       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-26       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-27       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-28       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-29       NaT      NaN              NaN  NaN            NaN   \n",
      "2025-09-30       NaT      NaN              NaN  NaN            NaN   \n",
      "\n",
      "            seuil_haut  seuil_bas  target  moyenne_mobile_7j  \\\n",
      "2025-09-21         NaN        NaN     NaN                NaN   \n",
      "2025-09-22         NaN        NaN     NaN                NaN   \n",
      "2025-09-23         NaN        NaN     NaN                NaN   \n",
      "2025-09-24         NaN        NaN     NaN                NaN   \n",
      "2025-09-25         NaN        NaN     NaN                NaN   \n",
      "2025-09-26         NaN        NaN     NaN                NaN   \n",
      "2025-09-27         NaN        NaN     NaN                NaN   \n",
      "2025-09-28         NaN        NaN     NaN                NaN   \n",
      "2025-09-29         NaN        NaN     NaN                NaN   \n",
      "2025-09-30         NaN        NaN     NaN                NaN   \n",
      "\n",
      "            moyenne_mobile_21j  ...  rsi_14j  PC1  PC2  PC3  PC4  \\\n",
      "2025-09-21                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-22                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-23                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-24                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-25                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-26                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-27                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-28                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-29                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "2025-09-30                 NaN  ...      NaN  NaN  NaN  NaN  NaN   \n",
      "\n",
      "            cluster_kmeans  inflation_eur  inflation_us  interest_rate_eur  \\\n",
      "2025-09-21             NaN       2.227488           NaN                2.0   \n",
      "2025-09-22             NaN       2.227488           NaN                2.0   \n",
      "2025-09-23             NaN       2.227488           NaN                2.0   \n",
      "2025-09-24             NaN       2.227488           NaN                2.0   \n",
      "2025-09-25             NaN       2.227488           NaN                2.0   \n",
      "2025-09-26             NaN       2.227488           NaN                2.0   \n",
      "2025-09-27             NaN       2.227488           NaN                2.0   \n",
      "2025-09-28             NaN       2.227488           NaN                2.0   \n",
      "2025-09-29             NaN       2.227488           NaN                2.0   \n",
      "2025-09-30             NaN       2.227488           NaN                2.0   \n",
      "\n",
      "            interest_rate_us  \n",
      "2025-09-21              4.25  \n",
      "2025-09-22              4.25  \n",
      "2025-09-23              4.25  \n",
      "2025-09-24              4.25  \n",
      "2025-09-25              4.25  \n",
      "2025-09-26              4.25  \n",
      "2025-09-27              4.25  \n",
      "2025-09-28              4.25  \n",
      "2025-09-29              4.25  \n",
      "2025-09-30              4.25  \n",
      "\n",
      "[10 rows x 23 columns]\n",
      "Index(['timestamp', 'eur_usd', 'taux_croissance', 'vol', 'rendement_10j',\n",
      "       'seuil_haut', 'seuil_bas', 'target', 'moyenne_mobile_7j',\n",
      "       'moyenne_mobile_21j', 'bollinger_moyenne', 'bollinger_haut',\n",
      "       'bollinger_bas', 'rsi_14j', 'PC1', 'PC2', 'PC3', 'PC4',\n",
      "       'cluster_kmeans', 'inflation_eur', 'inflation_us', 'interest_rate_eur',\n",
      "       'interest_rate_us'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.concat([df_extended, df_macro], axis=1)\n",
    "print(df_final.tail(10))\n",
    "print(df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac8013b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Variable cible correctement décalée de 2 semaines.\n",
      "Shape finale : (954, 18), target positive rate = 0.059\n",
      "Chargement des données...\n",
      "Tailles -> train: (667, 18), val: (143, 18), test: (144, 18)\n"
     ]
    },
    {
     "ename": "DTypePromotionError",
     "evalue": "The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int32DType'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDTypePromotionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 118\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Imputation (median) - fit uniquement sur train\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m    117\u001b[39m imputer = SimpleImputer(strategy=\u001b[33m'\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[43mimputer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m X_train_imp = pd.DataFrame(imputer.transform(X_train[num_cols]), index=X_train.index, columns=num_cols)\n\u001b[32m    121\u001b[39m X_val_imp   = pd.DataFrame(imputer.transform(X_val[num_cols]), index=X_val.index, columns=num_cols)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python D.Liron/fx-shock-prediction/venv/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python D.Liron/fx-shock-prediction/venv/lib/python3.12/site-packages/sklearn/impute/_base.py:452\u001b[39m, in \u001b[36mSimpleImputer.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    436\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the imputer on `X`.\u001b[39;00m\n\u001b[32m    437\u001b[39m \n\u001b[32m    438\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[39;00m\n\u001b[32m    455\u001b[39m     \u001b[38;5;66;03m# otherwise\u001b[39;00m\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python D.Liron/fx-shock-prediction/venv/lib/python3.12/site-packages/sklearn/impute/_base.py:360\u001b[39m, in \u001b[36mSimpleImputer._validate_input\u001b[39m\u001b[34m(self, X, in_fit)\u001b[39m\n\u001b[32m    357\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[32m    371\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcould not convert\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python D.Liron/fx-shock-prediction/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python D.Liron/fx-shock-prediction/venv/lib/python3.12/site-packages/sklearn/utils/validation.py:929\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    925\u001b[39m pandas_requires_conversion = \u001b[38;5;28many\u001b[39m(\n\u001b[32m    926\u001b[39m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[32m    927\u001b[39m )\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np.dtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     dtype_orig = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdtypes_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[32m    931\u001b[39m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[32m    932\u001b[39m     dtype_orig = \u001b[38;5;28mobject\u001b[39m\n",
      "\u001b[31mDTypePromotionError\u001b[39m: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int32DType'>)"
     ]
    }
   ],
   "source": [
    "# MODELISATION\n",
    "\n",
    "# xgb_single_model_timeseries.py\n",
    "\"\"\"\n",
    "XGBoost focalisé pour prédiction d'un choc à 2 semaines sur EUR/USD.\n",
    "Sorties :\n",
    "- AUC, Gini, courbe ROC (test),\n",
    "- seuil optimal (Youden) et matrice de confusion + métriques associées,\n",
    "- modèle final enregistré (joblib).\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------\n",
    "# Config \n",
    "# -----------------------\n",
    "RANDOM_STATE = 42\n",
    "# Fractions pour split contigu (doivent sommer à 1.0)\n",
    "TRAIN_FRAC = 0.70\n",
    "VAL_FRAC   = 0.15\n",
    "TEST_FRAC  = 0.15\n",
    "\n",
    "# TimeSeries CV splits (pour GridSearch)\n",
    "TS_SPLITS = 5\n",
    "\n",
    "# Grid search params \n",
    "XGB_PARAM_GRID = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Metric d'optimisation pour la recherche d'hyperparamètres (ici F1, tu peux changer en 'roc_auc')\n",
    "GRID_SCORING = 'f1'\n",
    "\n",
    "# Early stopping rounds pour re-entrainement final\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "# Fichiers de sortie\n",
    "MODEL_OUTPATH = \"xgb_final_model.joblib\"\n",
    "IMPUTER_OUTPATH = \"imputer.joblib\"\n",
    "\n",
    "# ===============================\n",
    "# Décalage de la variable cible pour prédire le choc à 2 semaines\n",
    "# ===============================\n",
    "\n",
    "SHIFT_DAYS = 10  # environ 2 semaines ouvrées\n",
    "\n",
    "# On crée une copie pour éviter les erreurs d’alignement\n",
    "df_final = df_extended.copy()\n",
    "\n",
    "# Décaler la target vers le passé : les features du jour t servent à prédire le choc à t+10\n",
    "df_final[\"target_future\"] = df_final[\"target\"].shift(-SHIFT_DAYS)\n",
    "\n",
    "# Supprimer les lignes où la cible future est manquante (en fin de série)\n",
    "df_final = df_final.dropna(subset=[\"target_future\"])\n",
    "\n",
    "# Définir X (features actuelles) et y (choc futur)\n",
    "X = df_final.drop(columns=[\"target\", \"target_future\"])  # on garde uniquement les variables explicatives\n",
    "y = df_final[\"target_future\"].astype(int)\n",
    "\n",
    "print(\"✅ Variable cible correctement décalée de 2 semaines.\")\n",
    "print(f\"Shape finale : {X.shape}, target positive rate = {y.mean():.3f}\")\n",
    "\n",
    "# -----------------------\n",
    "# Chargement & checks\n",
    "# -----------------------\n",
    "print(\"Chargement des données...\")\n",
    "\n",
    "df = df_final.copy().dropna().sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "# Features & target\n",
    "X = df.drop(columns=[\"target\", \"target_future\"])\n",
    "y = df[\"target_future\"].astype(int)\n",
    "\n",
    "# On suppose tout numérique — sinon adapter types/catégoriques\n",
    "num_cols = X.columns.tolist()\n",
    "\n",
    "# -----------------------\n",
    "# Split contigu (train / val / test)\n",
    "# -----------------------\n",
    "n = len(df)\n",
    "if not abs(TRAIN_FRAC + VAL_FRAC + TEST_FRAC - 1.0) < 1e-8:\n",
    "    raise ValueError(\"TRAIN_FRAC + VAL_FRAC + TEST_FRAC doit être égal à 1.0\")\n",
    "\n",
    "train_end = int(n * TRAIN_FRAC)\n",
    "val_end = train_end + int(n * VAL_FRAC)\n",
    "\n",
    "X_train = X.iloc[:train_end].copy()\n",
    "y_train = y.iloc[:train_end].copy()\n",
    "\n",
    "X_val = X.iloc[train_end:val_end].copy()\n",
    "y_val = y.iloc[train_end:val_end].copy()\n",
    "\n",
    "X_test = X.iloc[val_end:].copy()\n",
    "y_test = y.iloc[val_end:].copy()\n",
    "\n",
    "print(f\"Tailles -> train: {X_train.shape}, val: {X_val.shape}, test: {X_test.shape}\")\n",
    "\n",
    "# -----------------------\n",
    "# Imputation (median) - fit uniquement sur train\n",
    "# -----------------------\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(X_train[num_cols])\n",
    "\n",
    "X_train_imp = pd.DataFrame(imputer.transform(X_train[num_cols]), index=X_train.index, columns=num_cols)\n",
    "X_val_imp   = pd.DataFrame(imputer.transform(X_val[num_cols]), index=X_val.index, columns=num_cols)\n",
    "X_test_imp  = pd.DataFrame(imputer.transform(X_test[num_cols]), index=X_test.index, columns=num_cols)\n",
    "\n",
    "# Save imputer for reproducibility / production\n",
    "joblib.dump(imputer, IMPUTER_OUTPATH)\n",
    "print(f\"Imputer sauvegardé -> {IMPUTER_OUTPATH}\")\n",
    "\n",
    "# -----------------------\n",
    "# GridSearchCV (TimeSeriesSplit) pour chercher les meilleurs hyperparams\n",
    "# -----------------------\n",
    "print(\"\\nLancement GridSearchCV (TimeSeriesSplit) sur XGBoost...\")\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=TS_SPLITS)\n",
    "\n",
    "# GridSearchCV sur les features imputées (pas de scaling nécessaire pour XGBoost)\n",
    "gscv = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=XGB_PARAM_GRID,\n",
    "    scoring=GRID_SCORING,\n",
    "    cv=tscv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "gscv.fit(X_train_imp, y_train)\n",
    "print(\"Meilleurs paramètres trouvés (GridSearchCV):\")\n",
    "print(gscv.best_params_)\n",
    "print(f\"Best CV {GRID_SCORING}: {gscv.best_score_:.4f}\")\n",
    "\n",
    "# -----------------------\n",
    "# Ré-entraînement final avec early stopping sur l'échantillon de validation\n",
    "# -----------------------\n",
    "best_params = gscv.best_params_.copy()\n",
    "\n",
    "# Conserver paramètres choisis et activer early stopping via eval_set\n",
    "xgb_final = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE,\n",
    "                          n_jobs=-1, **best_params)\n",
    "\n",
    "print(\"\\nRé-entrainement final avec early stopping sur validation (eval_set)...\")\n",
    "xgb_final.fit(\n",
    "    X_train_imp, y_train,\n",
    "    eval_set=[(X_val_imp, y_val)],\n",
    "    early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Sauvegarde modèle\n",
    "joblib.dump(xgb_final, MODEL_OUTPATH)\n",
    "print(f\"Modèle final sauvegardé -> {MODEL_OUTPATH}\")\n",
    "\n",
    "# -----------------------\n",
    "# Prédiction out-of-sample (test) + évaluation\n",
    "# -----------------------\n",
    "print(\"\\nÉvaluation out-of-sample (test)...\")\n",
    "probs_test = xgb_final.predict_proba(X_test_imp)[:, 1]\n",
    "auc = roc_auc_score(y_test, probs_test)\n",
    "gini = 2*auc - 1\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs_test)\n",
    "\n",
    "# Seuil optimal - Youden (TPR - FPR maximisé)\n",
    "youden_idx = np.argmax(tpr - fpr)\n",
    "opt_threshold_youden = thresholds[youden_idx]\n",
    "\n",
    "# Seuil optimisant F1 (pour info)\n",
    "f1_scores = [f1_score(y_test, (probs_test >= t).astype(int), zero_division=0) for t in thresholds]\n",
    "opt_threshold_f1 = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# Choix du seuil final : tu peux choisir Youden ou F1 ; ici on utilise Youden tout en reportant F1-opt\n",
    "threshold = opt_threshold_youden\n",
    "\n",
    "preds = (probs_test >= threshold).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "\n",
    "precision = precision_score(y_test, preds, zero_division=0)\n",
    "recall = recall_score(y_test, preds, zero_division=0)            # sensitivity\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "f1 = f1_score(y_test, preds, zero_division=0)\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "bal_acc = balanced_accuracy_score(y_test, preds)\n",
    "\n",
    "# -----------------------\n",
    "# Affichage résultats (out-of-sample uniquement)\n",
    "# -----------------------\n",
    "print(f\"\\nRésultats (test) - XGBoost\")\n",
    "print(f\"AUC (ROC): {auc:.4f}\")\n",
    "print(f\"Gini: {gini:.4f}\")\n",
    "print(f\"Seuil Youden: {opt_threshold_youden:.4f} | Seuil F1-opt: {opt_threshold_f1:.4f}\")\n",
    "print(\"Matrice de confusion (tn, fp, fn, tp):\", (int(tn), int(fp), int(fn), int(tp)))\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"F1: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Balanced Accuracy: {bal_acc:.4f}\")\n",
    "\n",
    "# -----------------------\n",
    "# Tracer ROC (test)\n",
    "# -----------------------\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.plot(fpr, tpr, label=f\"XGBoost (AUC={auc:.3f})\")\n",
    "plt.plot([0,1],[0,1], linestyle='--', alpha=0.6)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC - Out-of-sample (test)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------\n",
    "# Tracer Matrice de confusion (test)\n",
    "# -----------------------\n",
    "cm = np.array([[tn, fp],[fn, tp]])\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f\"Matrice de confusion (test) - seuil={threshold:.4f}\")\n",
    "plt.ylabel(\"Vraie classe\")\n",
    "plt.xlabel(\"Classe prédite\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------\n",
    "# Importance des features (optionnel, utile pour interprétation)\n",
    "# -----------------------\n",
    "try:\n",
    "    imp = pd.Series(xgb_final.feature_importances_, index=num_cols).sort_values(ascending=False)\n",
    "    print(\"\\nTop 10 features par importance (XGBoost):\")\n",
    "    print(imp.head(10))\n",
    "    plt.figure(figsize=(6,4))\n",
    "    imp.head(15).plot(kind='bar')\n",
    "    plt.title(\"Feature importances (XGBoost)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -----------------------\n",
    "# Résumé final en dictionary (pratique pour reporting programmatique)\n",
    "# -----------------------\n",
    "report = {\n",
    "    'auc': float(auc),\n",
    "    'gini': float(gini),\n",
    "    'threshold_youden': float(opt_threshold_youden),\n",
    "    'threshold_f1': float(opt_threshold_f1),\n",
    "    'threshold_used': float(threshold),\n",
    "    'confusion': {'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)},\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'specificity': float(specificity),\n",
    "    'f1': float(f1),\n",
    "    'accuracy': float(accuracy),\n",
    "    'balanced_accuracy': float(bal_acc),\n",
    "    'best_params': best_params,\n",
    "    'trained_at': datetime.utcnow().isoformat() + 'Z'\n",
    "}\n",
    "\n",
    "# Enregistrer le reporting si souhaité\n",
    "pd.Series(report).to_json(\"xgb_report_test.json\")\n",
    "print(\"\\nReport JSON sauvegardé -> xgb_report_test.json\")\n",
    "print(\"\\nTerminé. Seuls les résultats out-of-sample (test) ont été affichés.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
